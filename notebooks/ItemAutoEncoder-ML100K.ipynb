{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_and_preprocess_movielens_100k_item_based(base_path='./', delimiter='\\t', batch_size=64):\n",
    "    \"\"\"\n",
    "    Load and preprocess the MovieLens 100K dataset for an item-based autoencoder model, including batching,\n",
    "    ensuring that the TensorFlow datasets are structured to provide (input, target) tuples where input and target are identical.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): The base path to the dataset files.\n",
    "        delimiter (str): The delimiter used in the dataset files.\n",
    "        batch_size (int): The size of batches to produce.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Contains the number of users, number of movies, TensorFlow dataset for training, and TensorFlow dataset for testing.\n",
    "    \"\"\"\n",
    "    # Load training and testing data\n",
    "    train_data = pd.read_csv(f'{base_path}movielens_100k_u1.base', sep=delimiter, header=None, names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "    test_data = pd.read_csv(f'{base_path}movielens_100k_u1.test', sep=delimiter, header=None, names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "\n",
    "    # Determine the number of users and movies\n",
    "    num_users = max(train_data['user_id'].max(), test_data['user_id'].max())\n",
    "    num_movies = max(train_data['movie_id'].max(), test_data['movie_id'].max())\n",
    "\n",
    "    # Convert to zero-based index for TensorFlow processing\n",
    "    train_data[['user_id', 'movie_id']] -= 1\n",
    "    test_data[['user_id', 'movie_id']] -= 1\n",
    "\n",
    "    # Create matrices\n",
    "    train_ratings_matrix = np.zeros((num_users, num_movies))\n",
    "    test_ratings_matrix = np.zeros((num_users, num_movies))\n",
    "\n",
    "    for row in train_data.itertuples():\n",
    "        train_ratings_matrix[row.user_id, row.movie_id] = row.rating\n",
    "    for row in test_data.itertuples():\n",
    "        test_ratings_matrix[row.user_id, row.movie_id] = row.rating\n",
    "\n",
    "    # Convert matrices to TensorFlow datasets, ensuring each item is mapped to (input, target) tuple\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_ratings_matrix.T, train_ratings_matrix.T))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_ratings_matrix.T, test_ratings_matrix.T))\n",
    "\n",
    "    # Convert datasets to float32, necessary for TensorFlow processing\n",
    "    train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))\n",
    "    test_dataset = test_dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))\n",
    "\n",
    "    # Shuffle the training dataset with a specified buffer size and batch both datasets\n",
    "    shuffle_buffer_size = num_movies  # Adjust based on available memory\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=shuffle_buffer_size).batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "    return num_users, num_movies, train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 943\n",
      "Number of movies: 1682\n",
      "Shape of training data inputs: (64, 943)\n",
      "Shape of training data targets: (64, 943)\n",
      "Shape of testing data inputs: (64, 943)\n",
      "Shape of testing data targets: (64, 943)\n"
     ]
    }
   ],
   "source": [
    "base_path = './data/MovieLens_100K/'  # Adjust this path to where the dataset files are located\n",
    "batch_size = 64  # Common choice for batch size, but can be adjusted based on memory capacity and model requirements\n",
    "\n",
    "# Load and preprocess the data\n",
    "num_users, num_movies, train_dataset, test_dataset = load_and_preprocess_movielens_100k_item_based(base_path=base_path, batch_size=batch_size)\n",
    "\n",
    "# Print out some insights\n",
    "print(f'Number of users: {num_users}')\n",
    "print(f'Number of movies: {num_movies}')\n",
    "\n",
    "# Iterate over the first batch to print its shape and get an insight into the batched dataset\n",
    "for inputs, targets in train_dataset.take(1):\n",
    "    print(f'Shape of training data inputs: {inputs.shape}')  # Shape should be (batch_size, num_movies)\n",
    "    print(f'Shape of training data targets: {targets.shape}')  # Shape should be identical to inputs\n",
    "\n",
    "for inputs, targets in test_dataset.take(1):\n",
    "    print(f'Shape of testing data inputs: {inputs.shape}')  # Shape should be (batch_size, num_movies)\n",
    "    print(f'Shape of testing data targets: {targets.shape}')  # Shape should be identical to inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Item_Autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_Layer (InputLayer)    [(None, 943)]             0         \n",
      "                                                                 \n",
      " Encoder_Layer (Dense)       (None, 500)               472000    \n",
      "                                                                 \n",
      " Decoder_Layer (Dense)       (None, 500)               250500    \n",
      "                                                                 \n",
      " Output_Layer (Dense)        (None, 943)               472443    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1194943 (4.56 MB)\n",
      "Trainable params: 1194943 (4.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Define RMSE as a metric.\"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def build_item_autoencoder(num_users, encoding_dim=128):\n",
    "    \"\"\"\n",
    "    Build an item-based autoencoder model.\n",
    "    \n",
    "    Args:\n",
    "        num_users (int): Number of users in the dataset to set the input dimension.\n",
    "        encoding_dim (int): Size of the encoding layer, representing the bottleneck.\n",
    "        \n",
    "    Returns:\n",
    "        A compiled Keras Model for the autoencoder.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(num_users,), name='Input_Layer')\n",
    "    \n",
    "    # Encoder layers\n",
    "    encoded = Dense(encoding_dim, activation='linear', name='Encoder_Layer')(input_layer)\n",
    "        \n",
    "    # Decoder layers\n",
    "    decoded = Dense(encoding_dim, activation='linear', name='Decoder_Layer')(encoded)\n",
    "    \n",
    "    # Output layer\n",
    "    output_layer = Dense(num_users, activation='linear', name='Output_Layer')(decoded)\n",
    "    \n",
    "    # Define the model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer, name='Item_Autoencoder')\n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=[root_mean_squared_error, 'mae'])\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Define the number of users and encoding dimension\n",
    "encoding_dim = 500\n",
    "\n",
    "# Build the item-based autoencoder\n",
    "autoencoder_model_item = build_item_autoencoder(num_users, encoding_dim)\n",
    "\n",
    "# Display the model summary to check the architecture\n",
    "autoencoder_model_item.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "27/27 [==============================] - 1s 12ms/step - loss: 0.8031 - root_mean_squared_error: 0.8853 - mae: 0.4877 - val_loss: 0.2200 - val_root_mean_squared_error: 0.3846 - val_mae: 0.2059\n",
      "Epoch 2/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.4390 - root_mean_squared_error: 0.6557 - mae: 0.3554 - val_loss: 0.1842 - val_root_mean_squared_error: 0.3519 - val_mae: 0.1819\n",
      "Epoch 3/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.3398 - root_mean_squared_error: 0.5821 - mae: 0.3111 - val_loss: 0.1656 - val_root_mean_squared_error: 0.3335 - val_mae: 0.1713\n",
      "Epoch 4/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.2823 - root_mean_squared_error: 0.5315 - mae: 0.2863 - val_loss: 0.1528 - val_root_mean_squared_error: 0.3204 - val_mae: 0.1671\n",
      "Epoch 5/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.2421 - root_mean_squared_error: 0.4896 - mae: 0.2701 - val_loss: 0.1431 - val_root_mean_squared_error: 0.3101 - val_mae: 0.1645\n",
      "Epoch 6/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.2093 - root_mean_squared_error: 0.4606 - mae: 0.2556 - val_loss: 0.1358 - val_root_mean_squared_error: 0.3019 - val_mae: 0.1621\n",
      "Epoch 7/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1944 - root_mean_squared_error: 0.4401 - mae: 0.2495 - val_loss: 0.1307 - val_root_mean_squared_error: 0.2959 - val_mae: 0.1616\n",
      "Epoch 8/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1727 - root_mean_squared_error: 0.4122 - mae: 0.2390 - val_loss: 0.1254 - val_root_mean_squared_error: 0.2898 - val_mae: 0.1602\n",
      "Epoch 9/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1539 - root_mean_squared_error: 0.3902 - mae: 0.2286 - val_loss: 0.1206 - val_root_mean_squared_error: 0.2840 - val_mae: 0.1579\n",
      "Epoch 10/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.1406 - root_mean_squared_error: 0.3751 - mae: 0.2208 - val_loss: 0.1163 - val_root_mean_squared_error: 0.2789 - val_mae: 0.1562\n",
      "Epoch 11/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1303 - root_mean_squared_error: 0.3608 - mae: 0.2148 - val_loss: 0.1132 - val_root_mean_squared_error: 0.2749 - val_mae: 0.1550\n",
      "Epoch 12/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1213 - root_mean_squared_error: 0.3491 - mae: 0.2086 - val_loss: 0.1101 - val_root_mean_squared_error: 0.2711 - val_mae: 0.1538\n",
      "Epoch 13/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1138 - root_mean_squared_error: 0.3370 - mae: 0.2037 - val_loss: 0.1074 - val_root_mean_squared_error: 0.2677 - val_mae: 0.1529\n",
      "Epoch 14/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1085 - root_mean_squared_error: 0.3296 - mae: 0.1997 - val_loss: 0.1049 - val_root_mean_squared_error: 0.2645 - val_mae: 0.1517\n",
      "Epoch 15/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.1012 - root_mean_squared_error: 0.3171 - mae: 0.1944 - val_loss: 0.1026 - val_root_mean_squared_error: 0.2614 - val_mae: 0.1505\n",
      "Epoch 16/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0964 - root_mean_squared_error: 0.3116 - mae: 0.1903 - val_loss: 0.1007 - val_root_mean_squared_error: 0.2589 - val_mae: 0.1497\n",
      "Epoch 17/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0986 - root_mean_squared_error: 0.3139 - mae: 0.1920 - val_loss: 0.0994 - val_root_mean_squared_error: 0.2572 - val_mae: 0.1490\n",
      "Epoch 18/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0933 - root_mean_squared_error: 0.3046 - mae: 0.1878 - val_loss: 0.0976 - val_root_mean_squared_error: 0.2548 - val_mae: 0.1482\n",
      "Epoch 19/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0873 - root_mean_squared_error: 0.2943 - mae: 0.1826 - val_loss: 0.0954 - val_root_mean_squared_error: 0.2518 - val_mae: 0.1467\n",
      "Epoch 20/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0841 - root_mean_squared_error: 0.2871 - mae: 0.1801 - val_loss: 0.0941 - val_root_mean_squared_error: 0.2501 - val_mae: 0.1463\n",
      "Epoch 21/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0789 - root_mean_squared_error: 0.2811 - mae: 0.1756 - val_loss: 0.0919 - val_root_mean_squared_error: 0.2472 - val_mae: 0.1446\n",
      "Epoch 22/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0769 - root_mean_squared_error: 0.2769 - mae: 0.1734 - val_loss: 0.0907 - val_root_mean_squared_error: 0.2454 - val_mae: 0.1440\n",
      "Epoch 23/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0748 - root_mean_squared_error: 0.2733 - mae: 0.1712 - val_loss: 0.0898 - val_root_mean_squared_error: 0.2440 - val_mae: 0.1435\n",
      "Epoch 24/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0728 - root_mean_squared_error: 0.2688 - mae: 0.1690 - val_loss: 0.0885 - val_root_mean_squared_error: 0.2422 - val_mae: 0.1429\n",
      "Epoch 25/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0746 - root_mean_squared_error: 0.2721 - mae: 0.1704 - val_loss: 0.0884 - val_root_mean_squared_error: 0.2419 - val_mae: 0.1426\n",
      "Epoch 26/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0746 - root_mean_squared_error: 0.2737 - mae: 0.1703 - val_loss: 0.0871 - val_root_mean_squared_error: 0.2404 - val_mae: 0.1423\n",
      "Epoch 27/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0727 - root_mean_squared_error: 0.2694 - mae: 0.1684 - val_loss: 0.0859 - val_root_mean_squared_error: 0.2385 - val_mae: 0.1411\n",
      "Epoch 28/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0712 - root_mean_squared_error: 0.2705 - mae: 0.1666 - val_loss: 0.0851 - val_root_mean_squared_error: 0.2374 - val_mae: 0.1404\n",
      "Epoch 29/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0794 - root_mean_squared_error: 0.2819 - mae: 0.1743 - val_loss: 0.0871 - val_root_mean_squared_error: 0.2400 - val_mae: 0.1438\n",
      "Epoch 30/1000\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0952 - root_mean_squared_error: 0.3068 - mae: 0.1848 - val_loss: 0.0869 - val_root_mean_squared_error: 0.2397 - val_mae: 0.1430\n",
      "Epoch 31/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0870 - root_mean_squared_error: 0.2931 - mae: 0.1808 - val_loss: 0.0857 - val_root_mean_squared_error: 0.2382 - val_mae: 0.1422\n",
      "Epoch 32/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0847 - root_mean_squared_error: 0.2897 - mae: 0.1779 - val_loss: 0.0849 - val_root_mean_squared_error: 0.2374 - val_mae: 0.1418\n",
      "Epoch 33/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0847 - root_mean_squared_error: 0.2913 - mae: 0.1766 - val_loss: 0.0836 - val_root_mean_squared_error: 0.2353 - val_mae: 0.1398\n",
      "Epoch 34/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0794 - root_mean_squared_error: 0.2826 - mae: 0.1732 - val_loss: 0.0826 - val_root_mean_squared_error: 0.2338 - val_mae: 0.1391\n",
      "Epoch 35/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0737 - root_mean_squared_error: 0.2710 - mae: 0.1685 - val_loss: 0.0817 - val_root_mean_squared_error: 0.2325 - val_mae: 0.1383\n",
      "Epoch 36/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0690 - root_mean_squared_error: 0.2626 - mae: 0.1632 - val_loss: 0.0807 - val_root_mean_squared_error: 0.2309 - val_mae: 0.1374\n",
      "Epoch 37/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0636 - root_mean_squared_error: 0.2515 - mae: 0.1581 - val_loss: 0.0795 - val_root_mean_squared_error: 0.2291 - val_mae: 0.1362\n",
      "Epoch 38/1000\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0596 - root_mean_squared_error: 0.2445 - mae: 0.1542 - val_loss: 0.0784 - val_root_mean_squared_error: 0.2276 - val_mae: 0.1354\n",
      "Epoch 39/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0562 - root_mean_squared_error: 0.2357 - mae: 0.1506 - val_loss: 0.0776 - val_root_mean_squared_error: 0.2264 - val_mae: 0.1350\n",
      "Epoch 40/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0541 - root_mean_squared_error: 0.2313 - mae: 0.1484 - val_loss: 0.0768 - val_root_mean_squared_error: 0.2252 - val_mae: 0.1344\n",
      "Epoch 41/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0516 - root_mean_squared_error: 0.2272 - mae: 0.1454 - val_loss: 0.0760 - val_root_mean_squared_error: 0.2239 - val_mae: 0.1337\n",
      "Epoch 42/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0528 - root_mean_squared_error: 0.2279 - mae: 0.1459 - val_loss: 0.0757 - val_root_mean_squared_error: 0.2233 - val_mae: 0.1334\n",
      "Epoch 43/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0508 - root_mean_squared_error: 0.2259 - mae: 0.1434 - val_loss: 0.0747 - val_root_mean_squared_error: 0.2219 - val_mae: 0.1323\n",
      "Epoch 44/1000\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0559 - root_mean_squared_error: 0.2352 - mae: 0.1488 - val_loss: 0.0752 - val_root_mean_squared_error: 0.2226 - val_mae: 0.1332\n",
      "Epoch 45/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0525 - root_mean_squared_error: 0.2289 - mae: 0.1459 - val_loss: 0.0743 - val_root_mean_squared_error: 0.2214 - val_mae: 0.1324\n",
      "Epoch 46/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0493 - root_mean_squared_error: 0.2218 - mae: 0.1419 - val_loss: 0.0735 - val_root_mean_squared_error: 0.2201 - val_mae: 0.1317\n",
      "Epoch 47/1000\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0484 - root_mean_squared_error: 0.2192 - mae: 0.1406 - val_loss: 0.0732 - val_root_mean_squared_error: 0.2195 - val_mae: 0.1315\n",
      "Epoch 48/1000\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0489 - root_mean_squared_error: 0.2198 - mae: 0.1408 - val_loss: 0.0728 - val_root_mean_squared_error: 0.2188 - val_mae: 0.1312\n",
      "Epoch 49/1000\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0491 - root_mean_squared_error: 0.2214 - mae: 0.1413 - val_loss: 0.0723 - val_root_mean_squared_error: 0.2181 - val_mae: 0.1307\n",
      "Epoch 50/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0498 - root_mean_squared_error: 0.2217 - mae: 0.1417 - val_loss: 0.0724 - val_root_mean_squared_error: 0.2183 - val_mae: 0.1311\n",
      "Epoch 51/1000\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0495 - root_mean_squared_error: 0.2229 - mae: 0.1415 - val_loss: 0.0715 - val_root_mean_squared_error: 0.2169 - val_mae: 0.1301\n",
      "Epoch 52/1000\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0488 - root_mean_squared_error: 0.2224 - mae: 0.1409 - val_loss: 0.0715 - val_root_mean_squared_error: 0.2169 - val_mae: 0.1305\n",
      "Epoch 53/1000\n",
      "27/27 [==============================] - 2s 54ms/step - loss: 0.0588 - root_mean_squared_error: 0.2398 - mae: 0.1497 - val_loss: 0.0727 - val_root_mean_squared_error: 0.2187 - val_mae: 0.1316\n",
      "Epoch 54/1000\n",
      "27/27 [==============================] - 1s 15ms/step - loss: 0.0597 - root_mean_squared_error: 0.2422 - mae: 0.1511 - val_loss: 0.0722 - val_root_mean_squared_error: 0.2181 - val_mae: 0.1316\n",
      "Epoch 55/1000\n",
      "27/27 [==============================] - 2s 65ms/step - loss: 0.0558 - root_mean_squared_error: 0.2352 - mae: 0.1484 - val_loss: 0.0715 - val_root_mean_squared_error: 0.2171 - val_mae: 0.1309\n",
      "Epoch 56/1000\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.0555 - root_mean_squared_error: 0.2355 - mae: 0.1485 - val_loss: 0.0713 - val_root_mean_squared_error: 0.2164 - val_mae: 0.1302\n",
      "Epoch 57/1000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 0.0597 - root_mean_squared_error: 0.2438 - mae: 0.1513 - val_loss: 0.0712 - val_root_mean_squared_error: 0.2165 - val_mae: 0.1301\n",
      "Epoch 58/1000\n",
      "27/27 [==============================] - 2s 55ms/step - loss: 0.0590 - root_mean_squared_error: 0.2418 - mae: 0.1503 - val_loss: 0.0712 - val_root_mean_squared_error: 0.2163 - val_mae: 0.1302\n",
      "Epoch 59/1000\n",
      "27/27 [==============================] - 3s 105ms/step - loss: 0.0559 - root_mean_squared_error: 0.2367 - mae: 0.1480 - val_loss: 0.0708 - val_root_mean_squared_error: 0.2158 - val_mae: 0.1299\n",
      "Epoch 60/1000\n",
      "27/27 [==============================] - 1s 13ms/step - loss: 0.0549 - root_mean_squared_error: 0.2330 - mae: 0.1473 - val_loss: 0.0701 - val_root_mean_squared_error: 0.2149 - val_mae: 0.1294\n",
      "Epoch 61/1000\n",
      "27/27 [==============================] - 2s 68ms/step - loss: 0.0508 - root_mean_squared_error: 0.2240 - mae: 0.1420 - val_loss: 0.0694 - val_root_mean_squared_error: 0.2136 - val_mae: 0.1282\n",
      "Epoch 62/1000\n",
      "27/27 [==============================] - 2s 83ms/step - loss: 0.0487 - root_mean_squared_error: 0.2201 - mae: 0.1398 - val_loss: 0.0688 - val_root_mean_squared_error: 0.2126 - val_mae: 0.1277\n",
      "Epoch 63/1000\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0472 - root_mean_squared_error: 0.2167 - mae: 0.1383 - val_loss: 0.0684 - val_root_mean_squared_error: 0.2119 - val_mae: 0.1273\n",
      "Epoch 64/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0467 - root_mean_squared_error: 0.2147 - mae: 0.1370 - val_loss: 0.0679 - val_root_mean_squared_error: 0.2111 - val_mae: 0.1266\n",
      "Epoch 65/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0433 - root_mean_squared_error: 0.2079 - mae: 0.1330 - val_loss: 0.0671 - val_root_mean_squared_error: 0.2099 - val_mae: 0.1257\n",
      "Epoch 66/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0423 - root_mean_squared_error: 0.2051 - mae: 0.1317 - val_loss: 0.0670 - val_root_mean_squared_error: 0.2098 - val_mae: 0.1260\n",
      "Epoch 67/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0415 - root_mean_squared_error: 0.2050 - mae: 0.1308 - val_loss: 0.0665 - val_root_mean_squared_error: 0.2088 - val_mae: 0.1252\n",
      "Epoch 68/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0438 - root_mean_squared_error: 0.2083 - mae: 0.1334 - val_loss: 0.0669 - val_root_mean_squared_error: 0.2095 - val_mae: 0.1260\n",
      "Epoch 69/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0454 - root_mean_squared_error: 0.2115 - mae: 0.1349 - val_loss: 0.0669 - val_root_mean_squared_error: 0.2095 - val_mae: 0.1260\n",
      "Epoch 70/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0441 - root_mean_squared_error: 0.2082 - mae: 0.1331 - val_loss: 0.0662 - val_root_mean_squared_error: 0.2085 - val_mae: 0.1251\n",
      "Epoch 71/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0437 - root_mean_squared_error: 0.2085 - mae: 0.1332 - val_loss: 0.0660 - val_root_mean_squared_error: 0.2080 - val_mae: 0.1249\n",
      "Epoch 72/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0425 - root_mean_squared_error: 0.2046 - mae: 0.1314 - val_loss: 0.0655 - val_root_mean_squared_error: 0.2072 - val_mae: 0.1243\n",
      "Epoch 73/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0415 - root_mean_squared_error: 0.2019 - mae: 0.1303 - val_loss: 0.0654 - val_root_mean_squared_error: 0.2069 - val_mae: 0.1244\n",
      "Epoch 74/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0409 - root_mean_squared_error: 0.2020 - mae: 0.1294 - val_loss: 0.0650 - val_root_mean_squared_error: 0.2064 - val_mae: 0.1238\n",
      "Epoch 75/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0411 - root_mean_squared_error: 0.2040 - mae: 0.1289 - val_loss: 0.0650 - val_root_mean_squared_error: 0.2062 - val_mae: 0.1239\n",
      "Epoch 76/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0521 - root_mean_squared_error: 0.2255 - mae: 0.1414 - val_loss: 0.0672 - val_root_mean_squared_error: 0.2100 - val_mae: 0.1270\n",
      "Epoch 77/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0512 - root_mean_squared_error: 0.2259 - mae: 0.1418 - val_loss: 0.0660 - val_root_mean_squared_error: 0.2082 - val_mae: 0.1255\n",
      "Epoch 78/1000\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0587 - root_mean_squared_error: 0.2391 - mae: 0.1472 - val_loss: 0.0668 - val_root_mean_squared_error: 0.2100 - val_mae: 0.1269\n",
      "Epoch 79/1000\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0554 - root_mean_squared_error: 0.2350 - mae: 0.1458 - val_loss: 0.0664 - val_root_mean_squared_error: 0.2090 - val_mae: 0.1263\n",
      "Epoch 80/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0558 - root_mean_squared_error: 0.2347 - mae: 0.1457 - val_loss: 0.0664 - val_root_mean_squared_error: 0.2091 - val_mae: 0.1264\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_root_mean_squared_error', patience=5, restore_best_weights=True)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder_model_item.fit(\n",
    "    train_dataset, \n",
    "    validation_data=test_dataset,  \n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 162ms/step\n",
      "2/2 [==============================] - 0s 35ms/step\n",
      "2/2 [==============================] - 0s 143ms/step\n",
      "2/2 [==============================] - 1s 189ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Test MSE: 2.1609668731689453\n",
      "Test RMSE: 1.4700227975845337\n",
      "Test MAE: 1.1303948163986206\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for predictions and true ratings\n",
    "all_predictions = []\n",
    "all_true_ratings = []\n",
    "\n",
    "# Iterate through the test dataset to predict ratings\n",
    "for inputs, targets in test_dataset:\n",
    "    predictions = autoencoder_model_item.predict(inputs)\n",
    "    all_predictions.append(predictions)\n",
    "    all_true_ratings.append(targets.numpy())\n",
    "\n",
    "# Concatenate all batched predictions and true ratings\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_true_ratings = np.concatenate(all_true_ratings, axis=0)\n",
    "\n",
    "# Filter out unrated items (assuming they are represented by zeros)\n",
    "rated_indices = np.where(all_true_ratings != 0)\n",
    "filtered_predictions = all_predictions[rated_indices]\n",
    "filtered_true_ratings = all_true_ratings[rated_indices]\n",
    "\n",
    "# Calculate RMSE, MSE, and MAE for rated items\n",
    "mse = np.mean((filtered_predictions - filtered_true_ratings) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(filtered_predictions - filtered_true_ratings))\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.5397\n",
      "Recall@10: 0.7295\n",
      "NDCG@10: 0.7922\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_ranking_metrics_item(predictions, true_ratings, k=10, relevance_threshold=4):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K, Recall@K, and NDCG@K with relevance threshold for an item-based autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        predictions (np.array): Predicted scores for items, shape (num_items, num_users).\n",
    "        true_ratings (np.array): True ratings, shape (num_items, num_users).\n",
    "        k (int): Number of top recommendations to evaluate.\n",
    "        relevance_threshold (float): Threshold above which items are considered relevant.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Precision@K, Recall@K, and NDCG@K scores averaged across all items.\n",
    "    \"\"\"\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    # Define a function to calculate DCG; used for both DCG@K and IDCG@K\n",
    "    def dcg(scores):\n",
    "        return np.sum((2**scores - 1) / np.log2(np.arange(2, scores.size + 2)))\n",
    "\n",
    "    for item_predictions, item_true_ratings in zip(predictions.T, true_ratings.T):\n",
    "        # Determine which users have rated this item and whether their ratings are considered relevant\n",
    "        rated_users = item_true_ratings > 0\n",
    "        relevant_users = item_true_ratings > relevance_threshold\n",
    "        \n",
    "        # If no users have rated this item or no users have relevant ratings, skip to avoid zero division\n",
    "        if not np.any(rated_users) or not np.any(relevant_users):\n",
    "            continue\n",
    "        \n",
    "        # Rank users based on the predicted scores for this item\n",
    "        top_k_indices = np.argsort(item_predictions)[-k:][::-1]\n",
    "        \n",
    "        # Calculate precision, recall\n",
    "        num_relevant_in_top_k = np.sum(relevant_users[top_k_indices])\n",
    "        num_relevant_total = np.sum(relevant_users)\n",
    "        \n",
    "        precision_at_k = num_relevant_in_top_k / k\n",
    "        recall_at_k = num_relevant_in_top_k / num_relevant_total\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        top_k_relevance = relevant_users[top_k_indices].astype(int)\n",
    "        top_k_dcg = dcg(top_k_relevance)\n",
    "        ideal_dcg = dcg(np.sort(relevant_users)[-k:][::-1].astype(int))  # Sort by true relevance\n",
    "        \n",
    "        ndcg_at_k = top_k_dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "        \n",
    "        precisions.append(precision_at_k)\n",
    "        recalls.append(recall_at_k)\n",
    "        ndcgs.append(ndcg_at_k)\n",
    "    \n",
    "    # Calculate the average across all items for which metrics were computed\n",
    "    metrics = {\n",
    "        \"Precision@K\": np.mean(precisions) if precisions else 0,\n",
    "        \"Recall@K\": np.mean(recalls) if recalls else 0,\n",
    "        \"NDCG@K\": np.mean(ndcgs) if ndcgs else 0\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage:\n",
    "k = 10\n",
    "relevance_threshold = 4\n",
    "metrics_item = calculate_ranking_metrics_item(all_predictions, all_true_ratings, k=k, relevance_threshold=relevance_threshold)\n",
    "print(f\"Precision@{k}: {metrics_item['Precision@K']:.4f}\")\n",
    "print(f\"Recall@{k}: {metrics_item['Recall@K']:.4f}\")\n",
    "print(f\"NDCG@{k}: {metrics_item['NDCG@K']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Parkinsons-TF",
   "language": "python",
   "name": "parkinsons-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
