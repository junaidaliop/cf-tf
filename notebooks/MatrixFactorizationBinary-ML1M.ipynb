{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_binarize_ml1m(filepath, threshold=4):\n",
    "    \"\"\"\n",
    "    Load the MovieLens 1M dataset, binarize ratings, and correctly handle user and movie IDs.\n",
    "    \n",
    "    Args:\n",
    "    - filepath (str): Path to the MovieLens 1M dataset.\n",
    "    - threshold (int): Threshold for binarizing ratings (ratings >= threshold are positive).\n",
    "    \n",
    "    Returns:\n",
    "    - ratings_matrix (np.ndarray): Binarized ratings matrix.\n",
    "    - user_id_mapping (dict): Mapping of original user IDs to matrix indices.\n",
    "    - movie_id_mapping (dict): Mapping of original movie IDs to matrix indices.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(filepath, sep='::', engine='python', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Binarize ratings\n",
    "    df['rating'] = (df['rating'] >= threshold).astype(int)\n",
    "    \n",
    "    # Create mappings for user and movie IDs\n",
    "    user_ids = df['user_id'].unique()\n",
    "    movie_ids = df['movie_id'].unique()\n",
    "    user_id_mapping = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "    movie_id_mapping = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "    \n",
    "    # Initialize ratings matrix\n",
    "    num_users, num_movies = len(user_ids), len(movie_ids)\n",
    "    ratings_matrix = np.zeros((num_users, num_movies), dtype=int)\n",
    "    \n",
    "    # Fill ratings matrix using mappings\n",
    "    for row in df.itertuples(index=False):\n",
    "        user_index = user_id_mapping[row.user_id]\n",
    "        movie_index = movie_id_mapping[row.movie_id]\n",
    "        ratings_matrix[user_index, movie_index] = row.rating\n",
    "    \n",
    "    return ratings_matrix, user_id_mapping, movie_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ratings(ratings_matrix, test_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split ratings into training and testing sets by masking a percentage of ratings.\n",
    "    \n",
    "    Args:\n",
    "    - ratings_matrix (np.ndarray): Full binarized ratings matrix.\n",
    "    - test_ratio (float): Fraction of ratings to use as the test set.\n",
    "    - random_state (int): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - train_matrix (np.ndarray): Training set ratings matrix.\n",
    "    - test_matrix (np.ndarray): Test set ratings matrix.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    mask = np.random.rand(*ratings_matrix.shape) < test_ratio\n",
    "    train_matrix = np.copy(ratings_matrix)\n",
    "    test_matrix = np.copy(ratings_matrix)\n",
    "\n",
    "    # Apply mask\n",
    "    train_matrix[mask] = 0\n",
    "    test_matrix[~mask] = 0\n",
    "\n",
    "    return train_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the full ratings matrix: (6040, 3706)\n",
      "Shape of the training matrix: (6040, 3706)\n",
      "Shape of the test matrix: (6040, 3706)\n",
      "Density of the full ratings matrix: 2.57%\n",
      "Number of positive ratings in the training set: 460208\n",
      "Number of positive ratings in the test set: 115073\n",
      "Average number of ratings per user in the training set: 76.19\n",
      "Average number of ratings per movie in the training set: 124.18\n",
      "Number of users with at least one rating in the training set: 6038 / 6040\n",
      "Number of movies with at least one rating in the training set: 3490 / 3706\n"
     ]
    }
   ],
   "source": [
    "# Load, binarize, and split the dataset\n",
    "ratings_matrix, user_id_mapping, movie_id_mapping = load_and_binarize_ml1m('./data/MovieLens_1M/movielens_1m_dataset.dat')\n",
    "train_matrix, test_matrix = split_ratings(ratings_matrix)\n",
    "\n",
    "# Print the shapes of the matrices\n",
    "print(\"Shape of the full ratings matrix:\", ratings_matrix.shape)\n",
    "print(\"Shape of the training matrix:\", train_matrix.shape)\n",
    "print(\"Shape of the test matrix:\", test_matrix.shape)\n",
    "\n",
    "# Calculate and print the density of the full ratings matrix\n",
    "non_zero_ratings = np.count_nonzero(ratings_matrix)\n",
    "total_possible_ratings = ratings_matrix.size\n",
    "density = (non_zero_ratings / total_possible_ratings) * 100\n",
    "print(f\"Density of the full ratings matrix: {density:.2f}%\")\n",
    "\n",
    "# Calculate and print the number of positive ratings in the training and test sets\n",
    "train_positives = np.count_nonzero(train_matrix)\n",
    "test_positives = np.count_nonzero(test_matrix)\n",
    "print(f\"Number of positive ratings in the training set: {train_positives}\")\n",
    "print(f\"Number of positive ratings in the test set: {test_positives}\")\n",
    "\n",
    "# Calculate and print the distribution of ratings across users and movies in the training set\n",
    "print(f\"Average number of ratings per user in the training set: {np.mean(np.count_nonzero(train_matrix, axis=1)):.2f}\")\n",
    "print(f\"Average number of ratings per movie in the training set: {np.mean(np.count_nonzero(train_matrix, axis=0)):.2f}\")\n",
    "\n",
    "# Verify that every user and movie has at least one rating in the training set\n",
    "users_with_ratings_train = np.any(train_matrix > 0, axis=1).sum()\n",
    "movies_with_ratings_train = np.any(train_matrix > 0, axis=0).sum()\n",
    "print(f\"Number of users with at least one rating in the training set: {users_with_ratings_train} / {train_matrix.shape[0]}\")\n",
    "print(f\"Number of movies with at least one rating in the training set: {movies_with_ratings_train} / {train_matrix.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "latent_dim = 32\n",
    "num_users, num_movies = ratings_matrix.shape\n",
    "\n",
    "# Inputs\n",
    "user_input = Input(shape=(1,), name='user_input', dtype='int32')\n",
    "item_input = Input(shape=(1,), name='item_input', dtype='int32')\n",
    "\n",
    "# Embeddings\n",
    "user_embedding = Embedding(num_users, latent_dim, name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(num_movies, latent_dim, name='item_embedding')(item_input)\n",
    "\n",
    "# Flatten embeddings\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "\n",
    "# Dot product of user and item embeddings\n",
    "dot_product = Dot(axes=1)([user_vec, item_vec])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# Model\n",
    "binary_model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "binary_model.compile(optimizer=Adam(0.001), loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_user_ids, train_item_ids = np.where(train_matrix > 0)\n",
    "train_labels = train_matrix[train_user_ids, train_item_ids]\n",
    "\n",
    "# Test data\n",
    "test_user_ids, test_item_ids = np.where(test_matrix > 0)\n",
    "test_labels = test_matrix[test_user_ids, test_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6472/6472 [==============================] - 26s 4ms/step - loss: 0.0928 - val_loss: 0.2356\n",
      "Epoch 2/5\n",
      "6472/6472 [==============================] - 24s 4ms/step - loss: 1.4545e-04 - val_loss: 0.2048\n",
      "Epoch 3/5\n",
      "6472/6472 [==============================] - 24s 4ms/step - loss: 7.1995e-06 - val_loss: 0.1661\n",
      "Epoch 4/5\n",
      "6472/6472 [==============================] - 24s 4ms/step - loss: 4.7586e-07 - val_loss: 0.1270\n",
      "Epoch 5/5\n",
      "6472/6472 [==============================] - 26s 4ms/step - loss: 4.7992e-08 - val_loss: 0.0944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6e56711450>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model.fit([train_user_ids, train_item_ids], train_labels, \n",
    "          epochs=5, batch_size=64, \n",
    "          validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def precision_at_k(true_matrix, predicted_scores_matrix, k=10):\n",
    "    precisions = []\n",
    "    \n",
    "    for user_id in range(true_matrix.shape[0]):\n",
    "        true_items = np.where(true_matrix[user_id] > 0)[0]\n",
    "        top_k_predictions = np.argsort(-predicted_scores_matrix[user_id])[:k]\n",
    "        \n",
    "        tp = len(set(true_items) & set(top_k_predictions))\n",
    "        if len(true_items) > 0:\n",
    "            precisions.append(tp / min(k, len(true_items)))\n",
    "        else:\n",
    "            precisions.append(0.0)\n",
    "    \n",
    "    return np.mean(precisions)\n",
    "\n",
    "def recall_at_k(true_matrix, predicted_scores_matrix, k=10):\n",
    "    recalls = []\n",
    "    \n",
    "    for user_id in range(true_matrix.shape[0]):\n",
    "        true_items = np.where(true_matrix[user_id] > 0)[0]\n",
    "        top_k_predictions = np.argsort(-predicted_scores_matrix[user_id])[:k]\n",
    "        \n",
    "        tp = len(set(true_items) & set(top_k_predictions))\n",
    "        if len(true_items) > 0:\n",
    "            recalls.append(tp / len(true_items))\n",
    "        else:\n",
    "            recalls.append(0.0)\n",
    "    \n",
    "    return np.mean(recalls)\n",
    "\n",
    "def calculate_ndcg_at_k(true_matrix, predicted_scores_matrix, k=10):\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    for user_id in range(true_matrix.shape[0]):\n",
    "        true_items = true_matrix[user_id]\n",
    "        scores = predicted_scores_matrix[user_id]\n",
    "        ndcg = ndcg_score([true_items], [scores], k=k)\n",
    "        ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699508/699508 [==============================] - 1136s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "def generate_prediction_matrix(model, num_users, num_movies):\n",
    "    user_indices, item_indices = np.where(np.ones((num_users, num_movies)))\n",
    "    predictions = model.predict([user_indices, item_indices]).flatten()\n",
    "    prediction_matrix = predictions.reshape(num_users, num_movies)\n",
    "    return prediction_matrix\n",
    "\n",
    "predicted_scores_matrix = generate_prediction_matrix(binary_model, num_users, num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 10: 0.0170\n",
      "Recall at 10: 0.0115\n",
      "NDCG at 10: 0.0138\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "precision = precision_at_k(test_matrix, predicted_scores_matrix, k=k)\n",
    "recall = recall_at_k(test_matrix, predicted_scores_matrix, k=k)\n",
    "ndcg = calculate_ndcg_at_k(test_matrix, predicted_scores_matrix, k=k)\n",
    "\n",
    "print(f\"Precision at {k}: {precision:.4f}\")\n",
    "print(f\"Recall at {k}: {recall:.4f}\")\n",
    "print(f\"NDCG at {k}: {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Parkinsons-TF",
   "language": "python",
   "name": "parkinsons-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
